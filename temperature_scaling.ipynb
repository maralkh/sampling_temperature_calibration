{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Temperature Scaling for Noisy Language Models\n",
    "\n",
    "## Weight vs Activation Noise Experiments\n",
    "\n",
    "This notebook explores where noise comes from in compressed models and how it affects optimal temperature.\n",
    "\n",
    "**Key Questions:**\n",
    "1. Does noise from weights or activations dominate?\n",
    "2. Are noise sources additive?\n",
    "3. How does this explain T* > 1 in quantized models?\n",
    "\n",
    "**Main Result:**\n",
    "$$T^* = \\sqrt{1 + \\alpha} = \\sqrt{1 + \\frac{\\sigma^2}{\\tau^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from contextlib import contextmanager\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Injection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def weight_noise_context(model, noise_scale: float):\n",
    "    \"\"\"Add noise to model weights temporarily.\"\"\"\n",
    "    if noise_scale == 0:\n",
    "        yield model, {}\n",
    "        return\n",
    "    \n",
    "    original_weights = {}\n",
    "    noise_info = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' not in name:\n",
    "            continue\n",
    "        if any(x in name.lower() for x in ['layernorm', 'ln', 'embed']):\n",
    "            continue\n",
    "        \n",
    "        original_weights[name] = param.data.clone()\n",
    "        weight_std = param.data.std().item()\n",
    "        noise = torch.randn_like(param.data) * noise_scale * weight_std\n",
    "        param.data.add_(noise)\n",
    "        noise_info[name] = {'weight_std': weight_std, 'noise_std': noise_scale * weight_std}\n",
    "    \n",
    "    try:\n",
    "        yield model, noise_info\n",
    "    finally:\n",
    "        for name, original in original_weights.items():\n",
    "            param = dict(model.named_parameters())[name]\n",
    "            param.data.copy_(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def activation_noise_context(model, noise_scale: float):\n",
    "    \"\"\"Add noise to activations (inputs to linear layers) during forward pass.\"\"\"\n",
    "    if noise_scale == 0:\n",
    "        yield model, {}\n",
    "        return\n",
    "    \n",
    "    hooks = []\n",
    "    noise_info = {'layers': 0}\n",
    "    \n",
    "    def add_noise_hook(module, args, kwargs):\n",
    "        # Add noise to INPUT of linear layer\n",
    "        x = args[0] if args else kwargs.get('input')\n",
    "        if x is None:\n",
    "            return args, kwargs\n",
    "        noise = torch.randn_like(x) * noise_scale * x.std().item()\n",
    "        noisy_x = x + noise\n",
    "        return (noisy_x,) + args[1:], kwargs\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            hooks.append(module.register_forward_pre_hook(add_noise_hook, with_kwargs=True))\n",
    "            noise_info['layers'] += 1\n",
    "    \n",
    "    try:\n",
    "        yield model, noise_info\n",
    "    finally:\n",
    "        for h in hooks:\n",
    "            h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def combined_noise_context(model, weight_noise: float, activation_noise: float):\n",
    "    \"\"\"Add noise to both weights and activation inputs.\"\"\"\n",
    "    \n",
    "    # Weight noise\n",
    "    original_weights = {}\n",
    "    if weight_noise > 0:\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' not in name:\n",
    "                continue\n",
    "            if any(x in name.lower() for x in ['layernorm', 'ln', 'embed']):\n",
    "                continue\n",
    "            original_weights[name] = param.data.clone()\n",
    "            noise = torch.randn_like(param.data) * weight_noise * param.data.std().item()\n",
    "            param.data.add_(noise)\n",
    "    \n",
    "    # Activation noise hooks on INPUT (pre-hook)\n",
    "    hooks = []\n",
    "    if activation_noise > 0:\n",
    "        def add_noise_hook(module, args, kwargs):\n",
    "            x = args[0] if args else kwargs.get('input')\n",
    "            if x is None:\n",
    "                return args, kwargs\n",
    "            noise = torch.randn_like(x) * activation_noise * x.std().item()\n",
    "            noisy_x = x + noise\n",
    "            return (noisy_x,) + args[1:], kwargs\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                hooks.append(module.register_forward_pre_hook(add_noise_hook, with_kwargs=True))\n",
    "    \n",
    "    try:\n",
    "        yield model, {'weight': weight_noise, 'activation': activation_noise}\n",
    "    finally:\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "        for name, original in original_weights.items():\n",
    "            dict(model.named_parameters())[name].data.copy_(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(model, tokenizer, prompt: str) -> torch.Tensor:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.logits[0, -1, :].float().cpu()\n",
    "\n",
    "\n",
    "def compute_statistics(logits_clean, logits_noisy):\n",
    "    tau_sq = logits_clean.var().item()\n",
    "    sigma_sq = (logits_noisy - logits_clean).var().item()\n",
    "    alpha = sigma_sq / tau_sq if tau_sq > 0 else 0\n",
    "    t_star = np.sqrt(1 + alpha)\n",
    "    return {'tau_sq': tau_sq, 'sigma_sq': sigma_sq, 'alpha': alpha, 't_star': t_star}\n",
    "\n",
    "\n",
    "def evaluate_temperatures(logits_clean, logits_noisy, temperatures):\n",
    "    \"\"\"Evaluate different temperatures using multiple metrics.\"\"\"\n",
    "    clean_probs = F.softmax(logits_clean, dim=-1)\n",
    "    correct_token = logits_clean.argmax().item()\n",
    "    results = {}\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        noisy_probs = F.softmax(logits_noisy / temp, dim=-1)\n",
    "        \n",
    "        # KL divergence (for reference)\n",
    "        kl_div = F.kl_div(noisy_probs.log(), clean_probs, reduction='sum').item()\n",
    "        \n",
    "        # JS divergence (symmetric, bounded)\n",
    "        m_probs = 0.5 * (clean_probs + noisy_probs)\n",
    "        js_div = 0.5 * F.kl_div(m_probs.log(), clean_probs, reduction='sum').item() + \\\n",
    "                 0.5 * F.kl_div(m_probs.log(), noisy_probs, reduction='sum').item()\n",
    "        \n",
    "        # Total Variation distance\n",
    "        tv_dist = 0.5 * (clean_probs - noisy_probs).abs().sum().item()\n",
    "        \n",
    "        # Probability of correct token\n",
    "        prob_correct = noisy_probs[correct_token].item()\n",
    "        \n",
    "        # Cross entropy: -sum(p_clean * log(p_noisy))\n",
    "        cross_entropy = -(clean_probs * noisy_probs.log()).sum().item()\n",
    "        \n",
    "        results[temp] = {\n",
    "            'kl_div': kl_div,\n",
    "            'js_div': js_div,\n",
    "            'tv_dist': tv_dist,\n",
    "            'prob_correct': prob_correct,\n",
    "            'cross_entropy': cross_entropy,\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROMPTS = [\n",
    "    \"Count the eggs: ðŸ¥šðŸ¥šðŸ¥šðŸ¥šðŸ¥š. How many eggs are there?\",\n",
    "    \"I have 3 apples and 4 oranges. How many fruits in total?\",\n",
    "    \"Count: 1, 2, 3, 4, 5, 6, 7. What's the last number?\",\n",
    "    \"There are 2 cats, 3 dogs, and 1 bird. How many animals?\",\n",
    "    \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "    \"A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\",\n",
    "    \"Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs. This increased the value of the house by 150%. How much profit did he make?\",\n",
    "    \"James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Clean Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing clean baseline logits...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing clean baseline logits...\")\n",
    "clean_logits = {prompt: get_logits(model, tokenizer, prompt) for prompt in TEST_PROMPTS}\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 1: Weight Noise Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight noise 0.000: Î± = 0.0000, T* = 1.0000\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 12.75 MiB is free. Process 60982 has 14.98 GiB memory in use. Process 102388 has 14.97 GiB memory in use. Process 132587 has 15.85 GiB memory in use. Process 164728 has 14.97 GiB memory in use. Process 275607 has 14.97 GiB memory in use. Process 313276 has 3.53 GiB memory in use. Of the allocated memory 2.97 GiB is allocated by PyTorch, and 67.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4002766629.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnoise_scale\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight_noise_scales\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mall_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mweight_noise_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_scale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnoisy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTEST_PROMPTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlogits_noisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3126635533.py\u001b[0m in \u001b[0;36mweight_noise_context\u001b[0;34m(model, noise_scale)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moriginal_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mweight_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnoise_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'weight_std'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweight_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'noise_std'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnoise_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight_std\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 12.75 MiB is free. Process 60982 has 14.98 GiB memory in use. Process 102388 has 14.97 GiB memory in use. Process 132587 has 15.85 GiB memory in use. Process 164728 has 14.97 GiB memory in use. Process 275607 has 14.97 GiB memory in use. Process 313276 has 3.53 GiB memory in use. Of the allocated memory 2.97 GiB is allocated by PyTorch, and 67.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "weight_noise_scales = [0.0, 0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "weight_results = []\n",
    "\n",
    "for noise_scale in weight_noise_scales:\n",
    "    all_stats = []\n",
    "    with weight_noise_context(model, noise_scale) as (noisy_model, _):\n",
    "        for prompt in TEST_PROMPTS:\n",
    "            logits_noisy = get_logits(noisy_model, tokenizer, prompt)\n",
    "            stats = compute_statistics(clean_logits[prompt], logits_noisy)\n",
    "            all_stats.append(stats)\n",
    "    \n",
    "    weight_results.append({\n",
    "        'noise_scale': noise_scale,\n",
    "        'alpha': np.mean([s['alpha'] for s in all_stats]),\n",
    "        't_star': np.mean([s['t_star'] for s in all_stats]),\n",
    "    })\n",
    "    print(f\"Weight noise {noise_scale:.3f}: Î± = {weight_results[-1]['alpha']:.4f}, T* = {weight_results[-1]['t_star']:.4f}\")\n",
    "\n",
    "df_weight = pd.DataFrame(weight_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Activation Noise Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_noise_scales = [0.0, 0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "activation_results = []\n",
    "\n",
    "for noise_scale in activation_noise_scales:\n",
    "    all_stats = []\n",
    "    with activation_noise_context(model, noise_scale) as (noisy_model, _):\n",
    "        for prompt in TEST_PROMPTS:\n",
    "            logits_noisy = get_logits(noisy_model, tokenizer, prompt)\n",
    "            stats = compute_statistics(clean_logits[prompt], logits_noisy)\n",
    "            all_stats.append(stats)\n",
    "    \n",
    "    activation_results.append({\n",
    "        'noise_scale': noise_scale,\n",
    "        'alpha': np.mean([s['alpha'] for s in all_stats]),\n",
    "        't_star': np.mean([s['t_star'] for s in all_stats]),\n",
    "    })\n",
    "    print(f\"Activation noise {noise_scale:.3f}: Î± = {activation_results[-1]['alpha']:.4f}, T* = {activation_results[-1]['t_star']:.4f}\")\n",
    "\n",
    "df_activation = pd.DataFrame(activation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Compare Weight vs Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Î± comparison\n",
    "ax1 = axes[0]\n",
    "ax1.plot(df_weight['noise_scale'], df_weight['alpha'], 'bo-', linewidth=2, markersize=8, label='Weight noise')\n",
    "ax1.plot(df_activation['noise_scale'], df_activation['alpha'], 'rs-', linewidth=2, markersize=8, label='Activation noise')\n",
    "ax1.set_xlabel('Noise Scale')\n",
    "ax1.set_ylabel('Logit Noise Ratio (Î±)')\n",
    "ax1.set_title('Weight vs Activation Noise: Effect on Î±')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# T* comparison\n",
    "ax2 = axes[1]\n",
    "ax2.plot(df_weight['noise_scale'], df_weight['t_star'], 'bo-', linewidth=2, markersize=8, label='Weight noise')\n",
    "ax2.plot(df_activation['noise_scale'], df_activation['t_star'], 'rs-', linewidth=2, markersize=8, label='Activation noise')\n",
    "ax2.set_xlabel('Noise Scale')\n",
    "ax2.set_ylabel('Optimal Temperature T*')\n",
    "ax2.set_title('Weight vs Activation Noise: Effect on T*')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('weight_vs_activation_noise.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratio at each noise level\n",
    "print(\"Activation/Weight Î± ratio at each noise scale:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for w, a in zip(weight_results, activation_results):\n",
    "    if w['noise_scale'] > 0:\n",
    "        ratio = a['alpha'] / (w['alpha'] + 1e-10)\n",
    "        print(f\"Noise {w['noise_scale']:.3f}: Activation Î± is {ratio:.1f}x Weight Î±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Combined Noise (Additivity Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory Validation: T* = âˆš(1 + Î±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate T* = sqrt(1 + Î±) for both noise types\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Theory curve\n",
    "alpha_range = np.linspace(0, max(df_activation['alpha'].max(), df_weight['alpha'].max()) * 1.1, 100)\n",
    "theory_t = np.sqrt(1 + alpha_range)\n",
    "\n",
    "# Left: Î± vs T* with theory\n",
    "ax1 = axes[0]\n",
    "ax1.plot(alpha_range, theory_t, 'k-', linewidth=2, label='Theory: $T^* = \\\\sqrt{1+\\\\alpha}$')\n",
    "ax1.scatter(df_weight['alpha'], df_weight['t_star'], s=100, c='blue', marker='o', label='Weight noise', zorder=5)\n",
    "ax1.scatter(df_activation['alpha'], df_activation['t_star'], s=100, c='red', marker='s', label='Activation noise', zorder=5)\n",
    "ax1.set_xlabel('Î± (noise-to-signal ratio)')\n",
    "ax1.set_ylabel('T*')\n",
    "ax1.set_title('Theory Validation: Both Noise Types')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Measured vs Predicted T*\n",
    "ax2 = axes[1]\n",
    "all_alpha = list(df_weight['alpha']) + list(df_activation['alpha'])\n",
    "all_t_measured = list(df_weight['t_star']) + list(df_activation['t_star'])\n",
    "all_t_predicted = [np.sqrt(1 + a) for a in all_alpha]\n",
    "colors = ['blue'] * len(df_weight) + ['red'] * len(df_activation)\n",
    "\n",
    "ax2.scatter(all_t_predicted, all_t_measured, c=colors, s=100, alpha=0.7)\n",
    "ax2.plot([1, max(all_t_predicted) * 1.05], [1, max(all_t_predicted) * 1.05], 'k--', linewidth=1, label='Perfect fit')\n",
    "ax2.set_xlabel('Predicted T* = âˆš(1+Î±)')\n",
    "ax2.set_ylabel('Measured T*')\n",
    "ax2.set_title('Predicted vs Measured')\n",
    "\n",
    "# Correlation\n",
    "corr = np.corrcoef(all_t_predicted, all_t_measured)[0, 1]\n",
    "ax2.text(0.05, 0.95, f'Correlation: {corr:.4f}', transform=ax2.transAxes, \n",
    "         fontsize=11, va='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('theory_validation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTheory validation:\")\n",
    "print(f\"  Correlation between predicted and measured T*: {corr:.4f}\")\n",
    "print(f\"  â†’ Formula T* = âˆš(1+Î±) holds for BOTH weight and activation noise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: KL Divergence Optimization\n",
    "\n",
    "Verify that T* minimizes KL divergence between noisy and clean distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different metrics across temperatures for both noise types\n",
    "temperatures = np.arange(0.8, 1.5, 0.02)\n",
    "noise_scale = 0.05  # Use higher noise to see clearer effect\n",
    "\n",
    "# Collect all metrics for each noise type\n",
    "metrics = ['kl_div', 'js_div', 'tv_dist', 'cross_entropy', 'prob_correct']\n",
    "\n",
    "# Weight noise\n",
    "weight_metrics = {m: {t: [] for t in temperatures} for m in metrics}\n",
    "with weight_noise_context(model, noise_scale) as (noisy_model, _):\n",
    "    for prompt in TEST_PROMPTS:\n",
    "        logits_noisy = get_logits(noisy_model, tokenizer, prompt)\n",
    "        logits_clean = clean_logits[prompt]\n",
    "        temp_results = evaluate_temperatures(logits_clean, logits_noisy, temperatures)\n",
    "        for t in temperatures:\n",
    "            for m in metrics:\n",
    "                weight_metrics[m][t].append(temp_results[t][m])\n",
    "\n",
    "weight_avg = {m: {t: np.mean(v) for t, v in weight_metrics[m].items()} for m in metrics}\n",
    "\n",
    "# Activation noise\n",
    "activation_metrics = {m: {t: [] for t in temperatures} for m in metrics}\n",
    "with activation_noise_context(model, noise_scale) as (noisy_model, _):\n",
    "    for prompt in TEST_PROMPTS:\n",
    "        logits_noisy = get_logits(noisy_model, tokenizer, prompt)\n",
    "        logits_clean = clean_logits[prompt]\n",
    "        temp_results = evaluate_temperatures(logits_clean, logits_noisy, temperatures)\n",
    "        for t in temperatures:\n",
    "            for m in metrics:\n",
    "                activation_metrics[m][t].append(temp_results[t][m])\n",
    "\n",
    "activation_avg = {m: {t: np.mean(v) for t, v in activation_metrics[m].items()} for m in metrics}\n",
    "\n",
    "# For backward compatibility\n",
    "weight_kl_avg = weight_avg['kl_div']\n",
    "activation_kl_avg = activation_avg['kl_div']\n",
    "\n",
    "print(\"All metrics computed for both noise types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted T* for this noise scale\n",
    "w_result = next((r for r in weight_results if abs(r['noise_scale'] - noise_scale) < 0.001), None)\n",
    "a_result = next((r for r in activation_results if abs(r['noise_scale'] - noise_scale) < 0.001), None)\n",
    "\n",
    "# Find best T for each metric\n",
    "print(\"Best T for each metric:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<15} {'Weight Best T':<15} {'Activation Best T':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_t_by_metric = {}\n",
    "for m in metrics:\n",
    "    if m == 'prob_correct':\n",
    "        # Maximize prob_correct\n",
    "        w_best = max(weight_avg[m].keys(), key=lambda t: weight_avg[m][t])\n",
    "        a_best = max(activation_avg[m].keys(), key=lambda t: activation_avg[m][t])\n",
    "    else:\n",
    "        # Minimize others\n",
    "        w_best = min(weight_avg[m].keys(), key=lambda t: weight_avg[m][t])\n",
    "        a_best = min(activation_avg[m].keys(), key=lambda t: activation_avg[m][t])\n",
    "    best_t_by_metric[m] = {'weight': w_best, 'activation': a_best}\n",
    "    print(f\"{m:<15} {w_best:<15.3f} {a_best:<15.3f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "if w_result:\n",
    "    print(f\"{'Predicted T*':<15} {w_result['t_star']:<15.3f} {a_result['t_star'] if a_result else 'N/A':<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all metrics for activation noise\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, m in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    temps = list(activation_avg[m].keys())\n",
    "    vals = list(activation_avg[m].values())\n",
    "    \n",
    "    ax.plot(temps, vals, 'r-', linewidth=2)\n",
    "    if a_result:\n",
    "        ax.axvline(x=a_result['t_star'], color='blue', linestyle='--', linewidth=2, label=f'Pred T*={a_result[\"t_star\"]:.3f}')\n",
    "    ax.axvline(x=best_t_by_metric[m]['activation'], color='green', linestyle=':', linewidth=2, label=f'Best={best_t_by_metric[m][\"activation\"]:.3f}')\n",
    "    ax.axvline(x=1.0, color='gray', linestyle='-', alpha=0.5)\n",
    "    ax.set_xlabel('Temperature')\n",
    "    ax.set_ylabel(m)\n",
    "    ax.set_title(f'{m}')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[5].axis('off')\n",
    "plt.suptitle(f'All Metrics vs Temperature (Activation Noise Ïƒ={noise_scale})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cross-Entropy as the main metric (more principled than KL)\n",
    "best_weight_t = best_t_by_metric['cross_entropy']['weight']\n",
    "best_activation_t = best_t_by_metric['cross_entropy']['activation']\n",
    "\n",
    "# Find closest to T=1.0\n",
    "t_one_weight = min(weight_kl_avg.keys(), key=lambda t: abs(t - 1.0))\n",
    "t_one_activation = min(activation_kl_avg.keys(), key=lambda t: abs(t - 1.0))\n",
    "\n",
    "print(f\"Using Cross-Entropy as metric:\")\n",
    "print(f\"  Weight:     Best T = {best_weight_t:.3f}, Predicted T* = {w_result['t_star']:.3f}\")\n",
    "print(f\"  Activation: Best T = {best_activation_t:.3f}, Predicted T* = {a_result['t_star']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined noise KL divergence\n",
    "combined_kl_results = {t: [] for t in temperatures}\n",
    "combined_stats = []\n",
    "\n",
    "with combined_noise_context(model, noise_scale, noise_scale) as (noisy_model, _):\n",
    "    for prompt in TEST_PROMPTS:\n",
    "        logits_noisy = get_logits(noisy_model, tokenizer, prompt)\n",
    "        logits_clean = clean_logits[prompt]\n",
    "        \n",
    "        # KL for each temperature\n",
    "        temp_results = evaluate_temperatures(logits_clean, logits_noisy, temperatures)\n",
    "        for t in temperatures:\n",
    "            combined_kl_results[t].append(temp_results[t]['kl_div'])\n",
    "        \n",
    "        # Stats (alpha, t_star)\n",
    "        stats = compute_statistics(logits_clean, logits_noisy)\n",
    "        combined_stats.append(stats)\n",
    "\n",
    "combined_kl_avg = {t: np.mean(kls) for t, kls in combined_kl_results.items()}\n",
    "combined_alpha = np.mean([s['alpha'] for s in combined_stats])\n",
    "combined_t_star = np.mean([s['t_star'] for s in combined_stats])\n",
    "best_combined_t = min(combined_kl_avg.keys(), key=lambda t: combined_kl_avg[t])\n",
    "t_one_combined = min(combined_kl_avg.keys(), key=lambda t: abs(t - 1.0))\n",
    "\n",
    "print(f\"Combined noise (W={noise_scale}, A={noise_scale}):\")\n",
    "print(f\"  Î± = {combined_alpha:.4f}\")\n",
    "print(f\"  Predicted T* = {combined_t_star:.3f}\")\n",
    "print(f\"  Best T (min KL) = {best_combined_t:.3f}\")\n",
    "\n",
    "# Sanity check\n",
    "print(f\"\\nSanity check:\")\n",
    "print(f\"  Weight Î±:     {w_result['alpha'] if w_result else 'N/A':.4f}\")\n",
    "print(f\"  Activation Î±: {a_result['alpha'] if a_result else 'N/A':.4f}\")\n",
    "print(f\"  Combined Î±:   {combined_alpha:.4f}\")\n",
    "print(f\"  Sum W+A:      {(w_result['alpha'] if w_result else 0) + (a_result['alpha'] if a_result else 0):.4f}\")\n",
    "if combined_alpha < (a_result['alpha'] if a_result else 0):\n",
    "    print(\"  âš ï¸ WARNING: Combined Î± < Activation Î±!\")\n",
    "    print(\"  â†’ This suggests weight noise REDUCES activation magnitudes,\")\n",
    "    print(\"    which then reduces the relative activation noise injection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all three: Weight, Activation, Combined\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Weight noise\n",
    "ax1 = axes[0]\n",
    "ax1.plot(list(weight_kl_avg.keys()), list(weight_kl_avg.values()), 'b-', linewidth=2)\n",
    "if w_result:\n",
    "    ax1.axvline(x=w_result['t_star'], color='red', linestyle='--', linewidth=2, label=f'Predicted T* = {w_result[\"t_star\"]:.3f}')\n",
    "ax1.axvline(x=best_weight_t, color='green', linestyle=':', linewidth=2, label=f'Best T = {best_weight_t:.3f}')\n",
    "ax1.axvline(x=1.0, color='gray', linestyle='-', alpha=0.5, label='T = 1.0')\n",
    "ax1.set_xlabel('Temperature')\n",
    "ax1.set_ylabel('KL Divergence')\n",
    "ax1.set_title(f'Weight Only (Ïƒ={noise_scale})')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Activation noise\n",
    "ax2 = axes[1]\n",
    "ax2.plot(list(activation_kl_avg.keys()), list(activation_kl_avg.values()), 'r-', linewidth=2)\n",
    "if a_result:\n",
    "    ax2.axvline(x=a_result['t_star'], color='red', linestyle='--', linewidth=2, label=f'Predicted T* = {a_result[\"t_star\"]:.3f}')\n",
    "ax2.axvline(x=best_activation_t, color='green', linestyle=':', linewidth=2, label=f'Best T = {best_activation_t:.3f}')\n",
    "ax2.axvline(x=1.0, color='gray', linestyle='-', alpha=0.5, label='T = 1.0')\n",
    "ax2.set_xlabel('Temperature')\n",
    "ax2.set_ylabel('KL Divergence')\n",
    "ax2.set_title(f'Activation Only (Ïƒ={noise_scale})')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Combined noise\n",
    "ax3 = axes[2]\n",
    "ax3.plot(list(combined_kl_avg.keys()), list(combined_kl_avg.values()), 'm-', linewidth=2)\n",
    "ax3.axvline(x=combined_t_star, color='red', linestyle='--', linewidth=2, label=f'Predicted T* = {combined_t_star:.3f}')\n",
    "ax3.axvline(x=best_combined_t, color='green', linestyle=':', linewidth=2, label=f'Best T = {best_combined_t:.3f}')\n",
    "ax3.axvline(x=1.0, color='gray', linestyle='-', alpha=0.5, label='T = 1.0')\n",
    "ax3.set_xlabel('Temperature')\n",
    "ax3.set_ylabel('KL Divergence')\n",
    "ax3.set_title(f'Combined (W={noise_scale}, A={noise_scale})')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('kl_divergence_all_three.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KL DIVERGENCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNoise scale: {noise_scale}\")\n",
    "print(f\"\\n{'Type':<15} {'Î±':<10} {'Pred T*':<10} {'Best T':<10} {'KL@T=1':<12} {'KL@Best':<12} {'Reduction':<10}\")\n",
    "print(\"-\" * 79)\n",
    "\n",
    "# Weight\n",
    "w_kl_1 = weight_kl_avg[t_one_weight]\n",
    "w_kl_best = weight_kl_avg[best_weight_t]\n",
    "w_reduction = (w_kl_1 - w_kl_best) / w_kl_1 * 100 if w_kl_1 > 0 else 0\n",
    "w_alpha = w_result['alpha'] if w_result else 0\n",
    "w_t = w_result['t_star'] if w_result else 1.0\n",
    "print(f\"{'Weight':<15} {w_alpha:<10.4f} {w_t:<10.3f} {best_weight_t:<10.3f} {w_kl_1:<12.4f} {w_kl_best:<12.4f} {w_reduction:<10.1f}%\")\n",
    "\n",
    "# Activation\n",
    "a_kl_1 = activation_kl_avg[t_one_activation]\n",
    "a_kl_best = activation_kl_avg[best_activation_t]\n",
    "a_reduction = (a_kl_1 - a_kl_best) / a_kl_1 * 100 if a_kl_1 > 0 else 0\n",
    "a_alpha = a_result['alpha'] if a_result else 0\n",
    "a_t = a_result['t_star'] if a_result else 1.0\n",
    "print(f\"{'Activation':<15} {a_alpha:<10.4f} {a_t:<10.3f} {best_activation_t:<10.3f} {a_kl_1:<12.4f} {a_kl_best:<12.4f} {a_reduction:<10.1f}%\")\n",
    "\n",
    "# Combined\n",
    "c_kl_1 = combined_kl_avg[t_one_combined]\n",
    "c_kl_best = combined_kl_avg[best_combined_t]\n",
    "c_reduction = (c_kl_1 - c_kl_best) / c_kl_1 * 100 if c_kl_1 > 0 else 0\n",
    "print(f\"{'Combined':<15} {combined_alpha:<10.4f} {combined_t_star:<10.3f} {best_combined_t:<10.3f} {c_kl_1:<12.4f} {c_kl_best:<12.4f} {c_reduction:<10.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(f\"  Combined Î± ({combined_alpha:.4f}) â‰ˆ Weight Î± ({w_alpha:.4f}) + Activation Î± ({a_alpha:.4f}) = {w_alpha + a_alpha:.4f}\")\n",
    "print(f\"  Combined needs higher T* ({combined_t_star:.3f}) than either alone\")\n",
    "print(f\"  Using correct T* reduces KL by {c_reduction:.1f}% vs T=1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5b: Sequence-Level Evaluation\n",
    "\n",
    "Single-token metrics don't capture error propagation during autoregressive generation.\n",
    "Here we evaluate temperature by generating full sequences and comparing to clean model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(model, tokenizer, prompt, max_tokens=20, temperature=1.0):\n",
    "    \"\"\"Generate a sequence with given temperature.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    generated_tokens = []\n",
    "    generated_logits = []\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[0, -1, :].float()\n",
    "            \n",
    "        # Apply temperature and sample\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        token = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        generated_tokens.append(token)\n",
    "        generated_logits.append(logits.cpu())\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[token]]).to(model.device)], dim=1)\n",
    "        \n",
    "        # Stop at EOS\n",
    "        if token == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return generated_tokens, generated_logits\n",
    "\n",
    "\n",
    "def generate_greedy(model, tokenizer, prompt, max_tokens=20):\n",
    "    \"\"\"Generate sequence greedily (deterministic).\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    generated_tokens = []\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        token = logits.argmax().item()\n",
    "        generated_tokens.append(token)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[token]]).to(model.device)], dim=1)\n",
    "        \n",
    "        if token == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clean reference sequences\n",
    "print(\"Generating clean reference sequences...\")\n",
    "clean_sequences = {}\n",
    "for prompt in TEST_PROMPTS:\n",
    "    clean_sequences[prompt] = generate_greedy(model, tokenizer, prompt, max_tokens=15)\n",
    "    decoded = tokenizer.decode(clean_sequences[prompt])\n",
    "    print(f\"  {prompt[:30]}... â†’ {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sequence_match(model, tokenizer, prompt, clean_tokens, temperature, noise_scale, num_samples=5):\n",
    "    \"\"\"\n",
    "    Evaluate how well noisy model + temperature matches clean sequence.\n",
    "    Uses GREEDY decoding (argmax after temperature scaling) for fair comparison.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    position_weighted = []\n",
    "    first_token_correct = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        with activation_noise_context(model, noise_scale) as (noisy_model, _):\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "            input_ids = inputs.input_ids\n",
    "            \n",
    "            gen_tokens = []\n",
    "            for t_idx in range(len(clean_tokens)):\n",
    "                with torch.no_grad():\n",
    "                    outputs = noisy_model(input_ids)\n",
    "                    logits = outputs.logits[0, -1, :].float()\n",
    "                \n",
    "                # Apply temperature then take argmax (greedy with temperature)\n",
    "                scaled_logits = logits / temperature\n",
    "                token = scaled_logits.argmax().item()\n",
    "                gen_tokens.append(token)\n",
    "                \n",
    "                # Use CLEAN token for next input (teacher forcing)\n",
    "                # This isolates per-token accuracy from error propagation\n",
    "                next_token = clean_tokens[t_idx]\n",
    "                input_ids = torch.cat([input_ids, torch.tensor([[next_token]]).to(model.device)], dim=1)\n",
    "            \n",
    "            # Token match rate\n",
    "            match_count = sum(1 for i in range(len(clean_tokens)) if gen_tokens[i] == clean_tokens[i])\n",
    "            matches.append(match_count / len(clean_tokens))\n",
    "            \n",
    "            # Position-weighted (early tokens matter more)\n",
    "            weights = [1.0 / (i + 1) for i in range(len(clean_tokens))]\n",
    "            weighted_match = sum(w * (gen_tokens[i] == clean_tokens[i]) for i, w in enumerate(weights))\n",
    "            position_weighted.append(weighted_match / sum(weights))\n",
    "            \n",
    "            # First token correct\n",
    "            first_token_correct.append(1 if gen_tokens[0] == clean_tokens[0] else 0)\n",
    "    \n",
    "    return {\n",
    "        'token_match': np.mean(matches),\n",
    "        'position_weighted': np.mean(position_weighted),\n",
    "        'first_token': np.mean(first_token_correct),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sequence matching across temperatures\n",
    "seq_temperatures = np.arange(0.8, 1.5, 0.05)\n",
    "num_samples = 10\n",
    "\n",
    "print(f\"Evaluating sequence matching (activation noise={noise_scale}, {num_samples} samples per T)...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "seq_results = {t: {'token_match': [], 'position_weighted': [], 'first_token': []} for t in seq_temperatures}\n",
    "\n",
    "for prompt in TEST_PROMPTS:\n",
    "    clean_tokens = clean_sequences[prompt]\n",
    "    print(f\"\\nPrompt: {prompt[:40]}...\")\n",
    "    \n",
    "    for temp in seq_temperatures:\n",
    "        # Create fresh context for each temperature\n",
    "        result = evaluate_sequence_match(\n",
    "            model, tokenizer, prompt, clean_tokens, temp,\n",
    "            noise_scale,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "        for k, v in result.items():\n",
    "            seq_results[temp][k].append(v)\n",
    "    \n",
    "    print(f\"  Done.\")\n",
    "\n",
    "# Average across prompts\n",
    "seq_avg = {t: {k: np.mean(v) for k, v in metrics.items()} for t, metrics in seq_results.items()}\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best T for each sequence metric\n",
    "best_seq_t = {}\n",
    "for metric in ['token_match', 'position_weighted', 'first_token']:\n",
    "    best_seq_t[metric] = max(seq_avg.keys(), key=lambda t: seq_avg[t][metric])\n",
    "\n",
    "print(\"Sequence-Level Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Predicted T* (from Î±): {a_result['t_star']:.3f}\")\n",
    "print(f\"\\nBest T by metric:\")\n",
    "for metric, best_t in best_seq_t.items():\n",
    "    print(f\"  {metric:<20}: T = {best_t:.3f} (score = {seq_avg[best_t][metric]:.3f})\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, metric in enumerate(['token_match', 'position_weighted', 'first_token']):\n",
    "    ax = axes[i]\n",
    "    temps = list(seq_avg.keys())\n",
    "    vals = [seq_avg[t][metric] for t in temps]\n",
    "    \n",
    "    ax.plot(temps, vals, 'ro-', linewidth=2, markersize=6)\n",
    "    ax.axvline(x=a_result['t_star'], color='blue', linestyle='--', linewidth=2, label=f'Predicted T*={a_result[\"t_star\"]:.3f}')\n",
    "    ax.axvline(x=best_seq_t[metric], color='green', linestyle=':', linewidth=2, label=f'Best T={best_seq_t[metric]:.3f}')\n",
    "    ax.axvline(x=1.0, color='gray', linestyle='-', alpha=0.5, label='T=1.0')\n",
    "    ax.set_xlabel('Temperature')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'{metric}')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Sequence-Level Evaluation (Activation Noise Ïƒ={noise_scale})', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sequence_level_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâ†’ Sequence-level best T is closer to predicted T* than single-token metrics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Combined Noise (Additivity Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if noise sources are additive\n",
    "test_scale = 0.02\n",
    "\n",
    "configs = [\n",
    "    ('Weight only', test_scale, 0.0),\n",
    "    ('Activation only', 0.0, test_scale),\n",
    "    ('Combined', test_scale, test_scale),\n",
    "]\n",
    "\n",
    "combined_results = []\n",
    "\n",
    "for name, w_noise, a_noise in configs:\n",
    "    all_stats = []\n",
    "    with combined_noise_context(model, w_noise, a_noise) as (noisy_model, _):\n",
    "        for prompt in TEST_PROMPTS:\n",
    "            logits_noisy = get_logits(noisy_model, tokenizer, prompt)\n",
    "            stats = compute_statistics(clean_logits[prompt], logits_noisy)\n",
    "            all_stats.append(stats)\n",
    "    \n",
    "    combined_results.append({\n",
    "        'name': name,\n",
    "        'weight_noise': w_noise,\n",
    "        'activation_noise': a_noise,\n",
    "        'alpha': np.mean([s['alpha'] for s in all_stats]),\n",
    "        't_star': np.mean([s['t_star'] for s in all_stats]),\n",
    "    })\n",
    "    print(f\"{name}: Î± = {combined_results[-1]['alpha']:.4f}, T* = {combined_results[-1]['t_star']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check additivity\n",
    "weight_alpha = combined_results[0]['alpha']\n",
    "activation_alpha = combined_results[1]['alpha']\n",
    "combined_alpha = combined_results[2]['alpha']\n",
    "expected_combined = weight_alpha + activation_alpha\n",
    "\n",
    "print(f\"\\nAdditivity Test (noise scale = {test_scale}):\")\n",
    "print(f\"  Weight Î±:              {weight_alpha:.4f}\")\n",
    "print(f\"  Activation Î±:          {activation_alpha:.4f}\")\n",
    "print(f\"  Expected (sum):        {expected_combined:.4f}\")\n",
    "print(f\"  Actual combined Î±:     {combined_alpha:.4f}\")\n",
    "print(f\"  Difference:            {abs(combined_alpha - expected_combined):.4f}\")\n",
    "\n",
    "if abs(combined_alpha - expected_combined) / expected_combined < 0.2:\n",
    "    print(\"\\nâ†’ Noise sources are approximately ADDITIVE\")\n",
    "else:\n",
    "    print(\"\\nâ†’ Noise sources have INTERACTION effects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize additivity\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "labels = ['Weight\\nonly', 'Activation\\nonly', 'Expected\\n(sum)', 'Actual\\ncombined']\n",
    "values = [weight_alpha, activation_alpha, expected_combined, combined_alpha]\n",
    "colors = ['#3498db', '#e74c3c', '#95a5a6', '#2ecc71']\n",
    "\n",
    "bars = ax.bar(labels, values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_ylabel('Î± (noise-to-signal ratio)')\n",
    "ax.set_title(f'Noise Source Additivity Test (scale = {test_scale})')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('noise_additivity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7: Detailed Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple configurations\n",
    "configs = [\n",
    "    ('Clean (baseline)', 0.0, 0.0),\n",
    "    ('Weight 0.01', 0.01, 0.0),\n",
    "    ('Weight 0.02', 0.02, 0.0),\n",
    "    ('Weight 0.05', 0.05, 0.0),\n",
    "    ('Activation 0.01', 0.0, 0.01),\n",
    "    ('Activation 0.02', 0.0, 0.02),\n",
    "    ('Activation 0.05', 0.0, 0.05),\n",
    "    ('Both 0.01', 0.01, 0.01),\n",
    "    ('Both 0.02', 0.02, 0.02),\n",
    "]\n",
    "\n",
    "full_results = []\n",
    "\n",
    "for name, w_noise, a_noise in configs:\n",
    "    all_stats = []\n",
    "    with combined_noise_context(model, w_noise, a_noise) as (noisy_model, _):\n",
    "        for prompt in TEST_PROMPTS:\n",
    "            logits_noisy = get_logits(noisy_model, tokenizer, prompt)\n",
    "            stats = compute_statistics(clean_logits[prompt], logits_noisy)\n",
    "            all_stats.append(stats)\n",
    "    \n",
    "    full_results.append({\n",
    "        'Configuration': name,\n",
    "        'Weight Ïƒ': w_noise,\n",
    "        'Activation Ïƒ': a_noise,\n",
    "        'Î±': np.mean([s['alpha'] for s in all_stats]),\n",
    "        'T*': np.mean([s['t_star'] for s in all_stats]),\n",
    "    })\n",
    "\n",
    "df_full = pd.DataFrame(full_results)\n",
    "df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8: Noise Evolution During Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_evolution(model, tokenizer, prompt, noise_type, noise_scale, max_steps=15):\n",
    "    \"\"\"Measure Î± at each generation step.\"\"\"\n",
    "    \n",
    "    # Get clean trajectory\n",
    "    clean_traj = []\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs.input_ids.clone()\n",
    "    \n",
    "    for t in range(max_steps):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits[0, -1, :].float().cpu()\n",
    "        token = logits.argmax().item()\n",
    "        clean_traj.append({'logits': logits, 'token': token})\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[token]]).to(model.device)], dim=1)\n",
    "    \n",
    "    # Measure with noise\n",
    "    alphas = []\n",
    "    \n",
    "    if noise_type == 'weight':\n",
    "        ctx = weight_noise_context(model, noise_scale)\n",
    "    else:\n",
    "        ctx = activation_noise_context(model, noise_scale)\n",
    "    \n",
    "    with ctx as (noisy_model, _):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        input_ids = inputs.input_ids.clone()\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            with torch.no_grad():\n",
    "                logits_noisy = noisy_model(input_ids).logits[0, -1, :].float().cpu()\n",
    "            \n",
    "            stats = compute_statistics(clean_traj[t]['logits'], logits_noisy)\n",
    "            alphas.append(stats['alpha'])\n",
    "            \n",
    "            token = clean_traj[t]['token']\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[token]]).to(model.device)], dim=1)\n",
    "    \n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Count from 1 to 10: 1, 2,\"\n",
    "noise_scale = 0.02\n",
    "\n",
    "weight_evolution = measure_evolution(model, tokenizer, prompt, 'weight', noise_scale)\n",
    "activation_evolution = measure_evolution(model, tokenizer, prompt, 'activation', noise_scale)\n",
    "\n",
    "print(f\"Weight noise evolution: {weight_evolution[0]:.4f} â†’ {weight_evolution[-1]:.4f}\")\n",
    "print(f\"Activation noise evolution: {activation_evolution[0]:.4f} â†’ {activation_evolution[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "steps = range(len(weight_evolution))\n",
    "ax.plot(steps, weight_evolution, 'bo-', linewidth=2, markersize=6, label='Weight noise')\n",
    "ax.plot(steps, activation_evolution, 'rs-', linewidth=2, markersize=6, label='Activation noise')\n",
    "\n",
    "# Trend lines\n",
    "w_slope = np.polyfit(steps, weight_evolution, 1)[0]\n",
    "a_slope = np.polyfit(steps, activation_evolution, 1)[0]\n",
    "\n",
    "ax.set_xlabel('Generation Step')\n",
    "ax.set_ylabel('Î± (noise ratio)')\n",
    "ax.set_title(f'Noise Evolution During Generation (scale = {noise_scale})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax.text(0.02, 0.98, f'Weight slope: {w_slope:.4f}\\nActivation slope: {a_slope:.4f}', \n",
    "        transform=ax.transAxes, va='top', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('noise_evolution_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find ratio at 0.02 scale\n",
    "w_02 = next((r for r in weight_results if r['noise_scale'] == 0.02), None)\n",
    "a_02 = next((r for r in activation_results if r['noise_scale'] == 0.02), None)\n",
    "\n",
    "if w_02 and a_02:\n",
    "    ratio = a_02['alpha'] / (w_02['alpha'] + 1e-10)\n",
    "    \n",
    "    print(f\"\\n1. DOMINANT NOISE SOURCE (at scale 0.02):\")\n",
    "    print(f\"   Weight noise Î±:     {w_02['alpha']:.4f} â†’ T* = {w_02['t_star']:.4f}\")\n",
    "    print(f\"   Activation noise Î±: {a_02['alpha']:.4f} â†’ T* = {a_02['t_star']:.4f}\")\n",
    "    print(f\"   Ratio: Activation is {ratio:.1f}x Weight\")\n",
    "    \n",
    "    if ratio > 2:\n",
    "        print(f\"\\n   â†’ ACTIVATION NOISE DOMINATES!\")\n",
    "        print(f\"   â†’ This explains why quantized models need T* > 1\")\n",
    "\n",
    "print(f\"\\n2. ADDITIVITY:\")\n",
    "print(f\"   Expected Î± (W + A): {expected_combined:.4f}\")\n",
    "print(f\"   Actual Î± (both):    {combined_alpha:.4f}\")\n",
    "print(f\"   â†’ Noise sources are {'additive' if abs(combined_alpha - expected_combined) / expected_combined < 0.2 else 'interactive'}\")\n",
    "\n",
    "print(f\"\\n3. EVOLUTION:\")\n",
    "print(f\"   Weight noise slope:     {w_slope:+.4f}\")\n",
    "print(f\"   Activation noise slope: {a_slope:+.4f}\")\n",
    "\n",
    "print(f\"\\n4. PRACTICAL RECOMMENDATIONS:\")\n",
    "print(f\"   For quantized models where T* â‰ˆ 1.2:\")\n",
    "print(f\"   â†’ Focus on activation quantization quality\")\n",
    "print(f\"   â†’ Weight quantization alone cannot explain high T*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Equations\n",
    "\n",
    "**Optimal Temperature:**\n",
    "$$T^* = \\sqrt{1 + \\alpha_{total}}$$\n",
    "\n",
    "**Where (approximately):**\n",
    "$$\\alpha_{total} \\approx \\alpha_{weight} + \\alpha_{activation}$$\n",
    "\n",
    "**Key Finding:**\n",
    "$$\\alpha_{activation} \\gg \\alpha_{weight}$$\n",
    "\n",
    "â†’ Activation noise dominates in quantized models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
