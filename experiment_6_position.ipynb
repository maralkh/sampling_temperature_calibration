{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: Noise Evolution & Position Analysis\n",
    "\n",
    "**Key Finding:** \u03b1 decreases with token position due to:\n",
    "1. LayerNorm washing out noise\n",
    "2. Attention dilution over longer context\n",
    "\n",
    "**Implication:** Dynamic T*(t) might be better than constant T*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# UTILITIES - Run this cell first!\n# ============================================================\n\n\"\"\"\nUtility functions for quantization noise and temperature experiments.\n\"\"\"\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom contextlib import contextmanager\nimport re\n\n\n# =============================================================================\n# Noise Injection\n# =============================================================================\n\n@contextmanager\ndef weight_noise_context(model, noise_scale):\n    \"\"\"\n    Context manager that adds Gaussian noise to model weights.\n    Noise is removed when exiting the context.\n    \"\"\"\n    original_weights = {}\n    \n    try:\n        # Add noise to weights\n        for name, param in model.named_parameters():\n            if 'weight' in name and param.requires_grad is False:\n                original_weights[name] = param.data.clone()\n                noise = torch.randn_like(param.data) * noise_scale * param.data.std()\n                param.data.add_(noise)\n        \n        yield model, original_weights\n        \n    finally:\n        # Restore original weights\n        for name, param in model.named_parameters():\n            if name in original_weights:\n                param.data.copy_(original_weights[name])\n\n\n@contextmanager  \ndef activation_noise_context(model, noise_scale):\n    \"\"\"\n    Context manager that adds Gaussian noise to activations via forward pre-hooks.\n    Uses INPUT hooks (pre-hook) for realistic quantization simulation.\n    \"\"\"\n    handles = []\n    \n    def make_hook(scale):\n        def hook(module, args, kwargs):\n            if len(args) > 0:\n                x = args[0]\n                noise = torch.randn_like(x) * scale * x.std()\n                noisy_x = x + noise\n                return (noisy_x,) + args[1:], kwargs\n            return args, kwargs\n        return hook\n    \n    try:\n        # Register pre-hooks on linear layers\n        for name, module in model.named_modules():\n            if isinstance(module, torch.nn.Linear):\n                handle = module.register_forward_pre_hook(make_hook(noise_scale), with_kwargs=True)\n                handles.append(handle)\n        \n        yield model, handles\n        \n    finally:\n        # Remove hooks\n        for handle in handles:\n            handle.remove()\n\n\n@contextmanager\ndef combined_noise_context(model, weight_noise_scale, activation_noise_scale):\n    \"\"\"\n    Context manager that adds both weight and activation noise.\n    \"\"\"\n    handles = []\n    original_weights = {}\n    \n    def make_hook(scale):\n        def hook(module, args, kwargs):\n            if len(args) > 0:\n                x = args[0]\n                noise = torch.randn_like(x) * scale * x.std()\n                noisy_x = x + noise\n                return (noisy_x,) + args[1:], kwargs\n            return args, kwargs\n        return hook\n    \n    try:\n        # Add weight noise\n        for name, param in model.named_parameters():\n            if 'weight' in name and param.requires_grad is False:\n                original_weights[name] = param.data.clone()\n                noise = torch.randn_like(param.data) * weight_noise_scale * param.data.std()\n                param.data.add_(noise)\n        \n        # Add activation hooks\n        for name, module in model.named_modules():\n            if isinstance(module, torch.nn.Linear):\n                handle = module.register_forward_pre_hook(make_hook(activation_noise_scale), with_kwargs=True)\n                handles.append(handle)\n        \n        yield model, (original_weights, handles)\n        \n    finally:\n        # Restore weights\n        for name, param in model.named_parameters():\n            if name in original_weights:\n                param.data.copy_(original_weights[name])\n        \n        # Remove hooks\n        for handle in handles:\n            handle.remove()\n\n\n# =============================================================================\n# Alpha (noise-to-signal ratio) Measurement\n# =============================================================================\n\ndef compute_alpha(clean_logits, noisy_logits):\n    \"\"\"\n    Compute noise-to-signal ratio \u03b1 = \u03c3\u00b2_noise / \u03c4\u00b2_signal\n    \n    Args:\n        clean_logits: Logits from clean model\n        noisy_logits: Logits from noisy model\n    \n    Returns:\n        dict with tau_sq, sigma_sq, alpha, t_star\n    \"\"\"\n    diff = noisy_logits - clean_logits\n    tau_sq = clean_logits.var().item()\n    sigma_sq = diff.var().item()\n    alpha = sigma_sq / tau_sq if tau_sq > 0 else 0\n    t_star = np.sqrt(1 + alpha)\n    \n    return {\n        'tau_sq': tau_sq,\n        'sigma_sq': sigma_sq,\n        'alpha': alpha,\n        't_star': t_star\n    }\n\n\ndef get_logits(model, tokenizer, prompt):\n    \"\"\"Get logits for next token prediction.\"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model(inputs.input_ids)\n    return outputs.logits[0, -1, :].float()\n\n\n# =============================================================================\n# Generation with Temperature (FIXED - uses sampling!)\n# =============================================================================\n\ndef generate_with_temperature(model, tokenizer, prompt, temperature=1.0, \n                              max_new_tokens=512, do_sample=True, seed=None):\n    \"\"\"\n    Generate text with proper temperature scaling.\n    \n    IMPORTANT: Temperature only affects sampling, not argmax!\n    - do_sample=True: temperature scales probabilities before sampling\n    - do_sample=False: temperature has NO effect (argmax is scale-invariant)\n    \n    Args:\n        model: The language model\n        tokenizer: The tokenizer\n        prompt: Input prompt string\n        temperature: Sampling temperature (only works with do_sample=True)\n        max_new_tokens: Maximum tokens to generate\n        do_sample: If True, sample from distribution. If False, greedy (T ignored!)\n        seed: Random seed for reproducibility\n    \n    Returns:\n        generated_text: The generated string\n        generated_tokens: List of token ids\n    \"\"\"\n    if seed is not None:\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(seed)\n    \n    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n    input_ids = inputs.input_ids.clone()\n    generated_tokens = []\n    \n    with torch.no_grad():\n        for _ in range(max_new_tokens):\n            outputs = model(input_ids)\n            logits = outputs.logits[0, -1, :].float()\n            \n            if do_sample and temperature > 0:\n                # Apply temperature and sample\n                scaled_logits = logits / temperature\n                probs = F.softmax(scaled_logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1).item()\n            else:\n                # Greedy - temperature has NO effect!\n                next_token = logits.argmax().item()\n            \n            generated_tokens.append(next_token)\n            input_ids = torch.cat([input_ids, torch.tensor([[next_token]]).to(model.device)], dim=1)\n            \n            if next_token == tokenizer.eos_token_id:\n                break\n    \n    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n    return generated_text, generated_tokens\n\n\ndef generate_greedy(model, tokenizer, prompt, max_new_tokens=512):\n    \"\"\"\n    Generate text with greedy decoding (deterministic).\n    Temperature has NO effect on greedy decoding!\n    \"\"\"\n    return generate_with_temperature(\n        model, tokenizer, prompt, \n        temperature=1.0, max_new_tokens=max_new_tokens, \n        do_sample=False\n    )\n\n\n# =============================================================================\n# Evaluation Metrics\n# =============================================================================\n\ndef evaluate_temperature_effect(clean_logits, noisy_logits, temperatures):\n    \"\"\"\n    Evaluate how different temperatures affect the noisy distribution.\n    \n    For each temperature, computes:\n    - KL divergence from clean distribution\n    - JS divergence (symmetric)\n    - Probability of correct (clean's argmax) token\n    - Cross entropy\n    \n    Args:\n        clean_logits: Logits from clean model\n        noisy_logits: Logits from noisy model  \n        temperatures: List of temperatures to test\n    \n    Returns:\n        dict mapping temperature to metrics\n    \"\"\"\n    clean_probs = F.softmax(clean_logits, dim=-1)\n    correct_token = clean_logits.argmax().item()\n    \n    results = {}\n    \n    for temp in temperatures:\n        noisy_probs = F.softmax(noisy_logits / temp, dim=-1)\n        \n        # KL divergence: KL(clean || noisy)\n        kl_div = F.kl_div(noisy_probs.log(), clean_probs, reduction='sum').item()\n        \n        # JS divergence (symmetric)\n        m_probs = 0.5 * (clean_probs + noisy_probs)\n        js_div = 0.5 * F.kl_div(m_probs.log(), clean_probs, reduction='sum').item() + \\\n                 0.5 * F.kl_div(m_probs.log(), noisy_probs, reduction='sum').item()\n        \n        # Probability of correct token\n        prob_correct = noisy_probs[correct_token].item()\n        \n        # Cross entropy\n        cross_entropy = -(clean_probs * noisy_probs.log()).sum().item()\n        \n        results[temp] = {\n            'kl_div': kl_div,\n            'js_div': js_div,\n            'prob_correct': prob_correct,\n            'cross_entropy': cross_entropy,\n        }\n    \n    return results\n\n\n# =============================================================================\n# GSM8K Helpers\n# =============================================================================\n\ndef extract_answer(text):\n    \"\"\"Extract numerical answer from model output.\"\"\"\n    # Look for #### pattern (GSM8K format)\n    match = re.search(r'####\\s*([\\d,\\.\\-]+)', text)\n    if match:\n        return match.group(1).replace(',', '')\n    \n    # Look for 'answer is X' pattern\n    match = re.search(r'answer is[:\\s]*([\\d,\\.\\-]+)', text.lower())\n    if match:\n        return match.group(1).replace(',', '')\n    \n    # Look for boxed answer (common in reasoning models)\n    match = re.search(r'\\\\boxed\\{([\\d,\\.\\-]+)\\}', text)\n    if match:\n        return match.group(1).replace(',', '')\n    \n    # Look for last number in text\n    numbers = re.findall(r'[\\d,]+\\.?\\d*', text)\n    if numbers:\n        return numbers[-1].replace(',', '')\n    \n    return None\n\n\ndef extract_ground_truth(answer_text):\n    \"\"\"Extract ground truth from GSM8K answer format.\"\"\"\n    match = re.search(r'####\\s*([\\d,\\.\\-]+)', answer_text)\n    if match:\n        return match.group(1).replace(',', '')\n    return None\n\n\ndef check_answer(pred, truth):\n    \"\"\"Check if predicted answer matches ground truth.\"\"\"\n    if pred is None or truth is None:\n        return False\n    try:\n        return abs(float(pred) - float(truth)) < 0.01\n    except:\n        return pred.strip() == truth.strip()\n\n\ndef format_gsm8k_prompt(question, tokenizer, use_chat_template=True):\n    \"\"\"Format GSM8K question as prompt.\"\"\"\n    if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n        messages = [\n            {\"role\": \"user\", \"content\": f\"Solve this math problem step by step:\\n{question}\"}\n        ]\n        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    else:\n        return f\"Question: {question}\\nLet me solve this step by step:\\n\"\n\n\n# =============================================================================\n# Reproducibility\n# =============================================================================\n\ndef set_seed(seed):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\n# =============================================================================\n# Sanity Checks\n# =============================================================================\n\ndef verify_noise_injection(model, tokenizer, noise_ctx_fn, noise_scale, prompt=\"Hello\"):\n    \"\"\"\n    Verify that noise injection is actually working.\n    \n    Returns True if noise is being applied (outputs differ).\n    \"\"\"\n    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n    \n    # Clean output\n    with torch.no_grad():\n        clean_out = model(inputs.input_ids).logits[0, -1, :]\n    \n    # Noisy outputs (should differ each time)\n    noisy_outs = []\n    for _ in range(3):\n        with noise_ctx_fn(model, noise_scale) as (noisy_model, _):\n            with torch.no_grad():\n                noisy_out = noisy_model(inputs.input_ids).logits[0, -1, :]\n                noisy_outs.append(noisy_out.clone())\n    \n    # Check that noisy differs from clean\n    diff_from_clean = (noisy_outs[0] - clean_out).abs().mean().item()\n    \n    # Check that noisy outputs differ from each other (stochastic)\n    diff_between_noisy = (noisy_outs[0] - noisy_outs[1]).abs().mean().item()\n    \n    print(f\"Mean diff from clean: {diff_from_clean:.4f}\")\n    print(f\"Mean diff between noisy runs: {diff_between_noisy:.4f}\")\n    \n    is_working = diff_from_clean > 0.01 and diff_between_noisy > 0.01\n    print(f\"Noise injection working: {is_working}\")\n    \n    return is_working\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  # Change as needed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "TEST_PROMPTS = [\n",
    "    \"The capital of France is\",\n",
    "    \"In mathematics, the number pi is approximately\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a: \u03b1 vs Token Position (Teacher Forcing)\n",
    "\n",
    "Measure \u03b1 at each position during autoregressive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_alpha_per_position(model, tokenizer, prompt, noise_ctx_fn, noise_scale, \n",
    "                                max_tokens=100, num_samples=3):\n",
    "    \"\"\"\n",
    "    Measure \u03b1 at each token position using teacher forcing.\n",
    "    \"\"\"\n",
    "    # First generate clean sequence\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    input_ids = inputs.input_ids.clone()\n",
    "    \n",
    "    clean_logits_seq = []\n",
    "    clean_tokens = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[0, -1, :].float()\n",
    "            clean_logits_seq.append(logits.cpu())\n",
    "            token = logits.argmax().item()\n",
    "            clean_tokens.append(token)\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[token]]).to(model.device)], dim=1)\n",
    "    \n",
    "    # Now measure \u03b1 at each position with noise\n",
    "    alpha_per_pos = [[] for _ in range(max_tokens)]\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        with noise_ctx_fn(model, noise_scale) as (noisy_model, _):\n",
    "            input_ids = inputs.input_ids.clone()\n",
    "            \n",
    "            for t in range(max_tokens):\n",
    "                with torch.no_grad():\n",
    "                    outputs = noisy_model(input_ids)\n",
    "                    noisy_logits = outputs.logits[0, -1, :].float().cpu()\n",
    "                \n",
    "                # Compute \u03b1\n",
    "                result = compute_alpha(clean_logits_seq[t], noisy_logits)\n",
    "                alpha_per_pos[t].append(result['alpha'])\n",
    "                \n",
    "                # Teacher forcing: use clean token\n",
    "                input_ids = torch.cat([input_ids, torch.tensor([[clean_tokens[t]]]).to(model.device)], dim=1)\n",
    "    \n",
    "    return [np.mean(alphas) for alphas in alpha_per_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure \u03b1 per position for both noise types\n",
    "noise_scale = 0.05\n",
    "max_tokens = 150\n",
    "\n",
    "print(f\"Measuring \u03b1 per position (noise={noise_scale}, tokens={max_tokens})...\")\n",
    "\n",
    "prompt = TEST_PROMPTS[0]\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "\n",
    "print(\"\\nActivation noise...\")\n",
    "alpha_activation = measure_alpha_per_position(\n",
    "    model, tokenizer, prompt, activation_noise_context, noise_scale, max_tokens\n",
    ")\n",
    "\n",
    "print(\"Weight noise...\")\n",
    "alpha_weight = measure_alpha_per_position(\n",
    "    model, tokenizer, prompt, weight_noise_context, noise_scale, max_tokens\n",
    ")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "positions = range(len(alpha_activation))\n",
    "\n",
    "# Left: Raw values\n",
    "ax1 = axes[0]\n",
    "ax1.plot(positions, alpha_activation, 'r-', alpha=0.7, linewidth=1.5, label='Activation')\n",
    "ax1.plot(positions, alpha_weight, 'b-', alpha=0.7, linewidth=1.5, label='Weight')\n",
    "ax1.set_xlabel('Token Position')\n",
    "ax1.set_ylabel('\u03b1')\n",
    "ax1.set_title('\u03b1 vs Token Position')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Smoothed\n",
    "window = 10\n",
    "act_smooth = np.convolve(alpha_activation, np.ones(window)/window, mode='valid')\n",
    "weight_smooth = np.convolve(alpha_weight, np.ones(window)/window, mode='valid')\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(len(act_smooth)), act_smooth, 'r-', linewidth=2, label='Activation (smoothed)')\n",
    "ax2.plot(range(len(weight_smooth)), weight_smooth, 'b-', linewidth=2, label='Weight (smoothed)')\n",
    "ax2.set_xlabel('Token Position')\n",
    "ax2.set_ylabel('\u03b1 (smoothed)')\n",
    "ax2.set_title(f'Smoothed \u03b1 (window={window})')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('alpha_vs_position.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nActivation noise:\")\n",
    "print(f\"  First 10: mean \u03b1 = {np.mean(alpha_activation[:10]):.4f}\")\n",
    "print(f\"  Last 10:  mean \u03b1 = {np.mean(alpha_activation[-10:]):.4f}\")\n",
    "print(f\"  Ratio: {np.mean(alpha_activation[:10])/np.mean(alpha_activation[-10:]):.1f}x decrease\")\n",
    "\n",
    "print(f\"\\nWeight noise:\")\n",
    "print(f\"  First 10: mean \u03b1 = {np.mean(alpha_weight[:10]):.4f}\")\n",
    "print(f\"  Last 10:  mean \u03b1 = {np.mean(alpha_weight[-10:]):.4f}\")\n",
    "print(f\"  Ratio: {np.mean(alpha_weight[:10])/np.mean(alpha_weight[-10:]):.1f}x decrease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b: \u03b1 vs Context Length (Fixed Context)\n",
    "\n",
    "Control for attention dilution by using fixed context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_alpha_fixed_context(model, tokenizer, prompt, noise_ctx_fn, noise_scale,\n",
    "                                 context_lengths=[10, 50, 100, 150], num_samples=5):\n",
    "    \"\"\"\n",
    "    Measure \u03b1 with fixed context length (controls for attention dilution).\n",
    "    \"\"\"\n",
    "    # First generate clean sequence\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    input_ids = inputs.input_ids.clone()\n",
    "    \n",
    "    max_len = max(context_lengths) + 10\n",
    "    clean_tokens = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            outputs = model(input_ids)\n",
    "            token = outputs.logits[0, -1, :].argmax().item()\n",
    "            clean_tokens.append(token)\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[token]]).to(model.device)], dim=1)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for ctx_len in context_lengths:\n",
    "        # Build context\n",
    "        context_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "        for t in range(ctx_len):\n",
    "            context_ids = torch.cat([context_ids, torch.tensor([[clean_tokens[t]]]).to(model.device)], dim=1)\n",
    "        \n",
    "        # Get clean logits\n",
    "        with torch.no_grad():\n",
    "            clean_logits = model(context_ids).logits[0, -1, :].float().cpu()\n",
    "        \n",
    "        # Get noisy logits\n",
    "        alphas = []\n",
    "        for _ in range(num_samples):\n",
    "            with noise_ctx_fn(model, noise_scale) as (noisy_model, _):\n",
    "                with torch.no_grad():\n",
    "                    noisy_logits = noisy_model(context_ids).logits[0, -1, :].float().cpu()\n",
    "                result = compute_alpha(clean_logits, noisy_logits)\n",
    "                alphas.append(result['alpha'])\n",
    "        \n",
    "        results.append({\n",
    "            'context_length': ctx_len,\n",
    "            'alpha_mean': np.mean(alphas),\n",
    "            'alpha_std': np.std(alphas),\n",
    "        })\n",
    "        print(f\"  Context {ctx_len}: \u03b1 = {np.mean(alphas):.4f} \u00b1 {np.std(alphas):.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fixed context\n",
    "context_lengths = [5, 10, 20, 50, 100, 150]\n",
    "\n",
    "print(\"Measuring \u03b1 with fixed context lengths...\")\n",
    "print(\"This isolates LayerNorm effect from attention dilution.\\n\")\n",
    "\n",
    "fixed_context_results = measure_alpha_fixed_context(\n",
    "    model, tokenizer, TEST_PROMPTS[0],\n",
    "    activation_noise_context, noise_scale,\n",
    "    context_lengths=context_lengths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare teacher forcing vs fixed context\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Teacher forcing (growing context)\n",
    "ax1 = axes[0]\n",
    "ax1.plot(range(len(alpha_activation)), alpha_activation, 'r-', alpha=0.7, linewidth=1.5)\n",
    "ax1.set_xlabel('Token Position')\n",
    "ax1.set_ylabel('\u03b1')\n",
    "ax1.set_title('Teacher Forcing (growing context)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Fixed context\n",
    "ax2 = axes[1]\n",
    "ctx_lens = [r['context_length'] for r in fixed_context_results]\n",
    "alphas = [r['alpha_mean'] for r in fixed_context_results]\n",
    "stds = [r['alpha_std'] for r in fixed_context_results]\n",
    "ax2.errorbar(ctx_lens, alphas, yerr=stds, fmt='bo-', linewidth=2, markersize=8, capsize=5)\n",
    "ax2.set_xlabel('Context Length')\n",
    "ax2.set_ylabel('\u03b1')\n",
    "ax2.set_title('Fixed Context (single prediction)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('teacher_forcing_vs_fixed_context.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "tf_ratio = np.mean(alpha_activation[:10]) / np.mean(alpha_activation[-10:])\n",
    "fc_ratio = fixed_context_results[0]['alpha_mean'] / fixed_context_results[-1]['alpha_mean']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Teacher forcing \u03b1 decrease: {tf_ratio:.1f}x\")\n",
    "print(f\"Fixed context \u03b1 decrease:   {fc_ratio:.1f}x\")\n",
    "print(f\"\\nConclusion: \", end=\"\")\n",
    "if fc_ratio > 1.5:\n",
    "    print(\"Attention dilution contributes to \u03b1 decrease\")\n",
    "else:\n",
    "    print(\"LayerNorm is the main cause of \u03b1 decrease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6c: Dynamic T*(t) vs Constant T*\n",
    "\n",
    "Compare position-dependent temperature with constant temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute T* per position\n",
    "t_star_per_pos = [np.sqrt(1 + a) for a in alpha_activation]\n",
    "\n",
    "# Plot T* vs position\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(positions, t_star_per_pos, 'b-', linewidth=2, label='T*(t) = \u221a(1+\u03b1(t))')\n",
    "ax.axhline(y=np.sqrt(1 + np.mean(alpha_activation)), color='red', linestyle='--', \n",
    "           label=f'Constant T* = {np.sqrt(1 + np.mean(alpha_activation)):.3f}')\n",
    "ax.axhline(y=1.0, color='gray', linestyle=':', alpha=0.5, label='T = 1.0')\n",
    "\n",
    "ax.set_xlabel('Token Position')\n",
    "ax.set_ylabel('T*')\n",
    "ax.set_title('Optimal Temperature vs Position')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('t_star_vs_position.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nT* range: {min(t_star_per_pos):.3f} to {max(t_star_per_pos):.3f}\")\n",
    "print(f\"Mean T*: {np.mean(t_star_per_pos):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings from this experiment:\n",
    "\n",
    "1. **\u03b1 decreases with position** - up to 15x from first to last tokens\n",
    "2. **Both LayerNorm and attention dilution** contribute to this decrease\n",
    "3. **Dynamic T*(t)** should outperform constant T*\n",
    "4. **For long sequences**, T \u2248 1 is reasonable since \u03b1 converges to low values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}